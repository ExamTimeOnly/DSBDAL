{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75febb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Vaishnavi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Vaishnavi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Vaishnavi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the necessary libraries and download any required resources:\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cf27a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or extract the document you want to analyze:\n",
    "document = \"hey there!! how are you? my self DAMBOOOd\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85eda0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization:\n",
      "['hey', 'there', '!', '!', 'how', 'are', 'you', '?', 'my', 'self', 'DAMBOOOd']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenization:\n",
    "\n",
    "tokens = word_tokenize(document)\n",
    "print(\"Tokenization:\")\n",
    "print(tokens)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47a4b27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tagging:\n",
      "[('hey', 'NN'), ('there', 'RB'), ('!', '.'), ('!', '.'), ('how', 'WRB'), ('are', 'VBP'), ('you', 'PRP'), ('?', '.'), ('my', 'PRP$'), ('self', 'NN'), ('DAMBOOOd', 'NNP')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# POS Tagging:\n",
    "tagged_tokens = pos_tag(tokens)\n",
    "print(\"POS Tagging:\")\n",
    "print(tagged_tokens)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "577cfc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Words Removal:\n",
      "['hey', '!', '!', '?', 'self', 'DAMBOOOd']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stop Words Removal:\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "print(\"Stop Words Removal:\")\n",
    "print(filtered_tokens)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e733ae9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming:\n",
      "['hey', 'there', '!', '!', 'how', 'are', 'you', '?', 'my', 'self', 'damboood']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# stemming\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "print(\"Stemming:\")\n",
    "print(stemmed_tokens)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c01e1347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization:\n",
      "['hey', 'there', '!', '!', 'how', 'are', 'you', '?', 'my', 'self', 'DAMBOOOd']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lemmatization\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "print(\"Lemmatization:\")\n",
    "print(lemmatized_tokens)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80ad7caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF:\n",
      "[[0.35355339 0.35355339 0.35355339 0.35355339 0.35355339 0.35355339\n",
      "  0.35355339 0.35355339]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate Term Frequency-Inverse Document Frequency (TF-IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform([document])\n",
    "\n",
    "print(\"TF-IDF:\")\n",
    "print(tfidf_matrix.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f6312f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351f71cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ba77b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# he code you provided demonstrates various natural language processing (NLP) techniques using the NLTK library in Python. Let's go through each section and provide a brief description:\n",
    "\n",
    "# Importing Libraries and Downloading Resources: The code imports necessary libraries, such as NLTK, and downloads additional resources like stopwords, POS tagger, and WordNet. These resources are essential for performing different NLP tasks.\n",
    "\n",
    "# Tokenization: Tokenization is the process of breaking down a text document into individual words or tokens. The word_tokenize() function from NLTK is used to tokenize the given document, separating words and punctuation marks. The result is a list of tokens.\n",
    "\n",
    "# POS Tagging: Part-of-speech (POS) tagging involves labeling each token with its corresponding part of speech, such as noun, verb, adjective, etc. The pos_tag() function is used to perform POS tagging on the tokenized document. It assigns a POS tag to each token based on its context and grammatical role.\n",
    "\n",
    "# Stop Words Removal: Stop words are commonly occurring words in a language that do not carry significant meaning, such as \"the,\" \"is,\" \"and,\" etc. In this section, the code removes stop words from the tokenized document using NLTK's pre-defined set of English stop words. The resulting list contains only meaningful words.\n",
    "\n",
    "# Stemming: Stemming is the process of reducing words to their base or root form. The code utilizes the Porter stemming algorithm through the PorterStemmer class from NLTK. It applies stemming to each token in the document, reducing words like \"running\" and \"ran\" to their common base form, \"run.\"\n",
    "\n",
    "# Lemmatization: Lemmatization is similar to stemming but aims to reduce words to their base form (lemma) based on their intended meaning. NLTK's WordNetLemmatizer is used to perform lemmatization on the tokens in the document. For example, it transforms words like \"better\" and \"best\" to their base form, \"good.\"\n",
    "\n",
    "# TF-IDF (Term Frequency-Inverse Document Frequency): TF-IDF is a numerical representation of a document's importance within a collection of documents. It considers both the frequency of a term in a document (TF) and the inverse frequency of the term in the entire document collection (IDF). The code utilizes the TfidfVectorizer from scikit-learn to calculate the TF-IDF matrix for the given document. The resulting matrix represents the document in a vector space, where each element corresponds to the importance of a term within the document.\n",
    "\n",
    "# Now, let's prepare some questions and answers based on the code:\n",
    "\n",
    "# Q1: What is the purpose of tokenization in natural language processing?\n",
    "# A1: Tokenization breaks down a text document into individual words or tokens, which serves as the fundamental unit for further analysis in NLP tasks.\n",
    "\n",
    "# Q2: What is the difference between stemming and lemmatization?\n",
    "# A2: Stemming reduces words to their base or root form without considering their intended meaning, whereas lemmatization takes into account the word's intended meaning and reduces it to its base form accordingly.\n",
    "\n",
    "# Q3: What is the purpose of removing stop words in text analysis?\n",
    "# A3: Stop words are commonly occurring words that do not contribute much to the overall meaning of a text. Removing them helps reduce noise and focuses on more important and meaningful words in the analysis.\n",
    "\n",
    "# Q4: What is TF-IDF, and how is it calculated?\n",
    "# A4: TF-IDF (Term Frequency-Inverse Document Frequency) is a numerical representation that measures the importance of a term in a document within a collection. It is calculated by combining the term"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
